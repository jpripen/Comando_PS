{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Titanic Project\n## by Comando PS"},{"metadata":{},"cell_type":"markdown","source":"## 1. Table of contents\n\n1. Table of Contents  \n2. [Introduction](#Introduction)  \n3. [Business Understanding](#Business_understanding)  \n    3.1. [Background](#Background)  \n    3.2. [Problem description](#Problem_description)  \n    3.3. [Target audience](#Target_audience)  \n    3.4. [Success criteria](#Success_criteria)  \n4. [Data Understanding](#Data_understanding)  \n    4.1. [Structure of training and test sets](#Data_structure)  \n    4.2. [Submission set](#Submission_set)  \n    4.3. [Exploratory data analysis](#EDA)  \n        4.3.1. Survival\n        4.3.2. Sex\n        4.3.3. Age\n5. [Data preparation](#Data_preparation)  \n    5.1. [Feature engineering](#Feature_engineering)  \n        5.1.1. data  \n        5.1.2. data\n        5.1.3. data\n        5.1.4. data\n    5.2 [Data wrangling](#Data_wrangling)\n6. [Modeling](#Modeling)  \n7. [Evaluation](#Evaluation)\n8. [Conclusion](#Conclusion)\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<a id='Introduction'></a>\n## 2. Introduction\n\nIn this notebook we will try to complete the introductory project for kaggle, which is trying to predict if a group of passengers on board of RMS Titanic survived or not the [tragic sinking of the ship in the early hours of 15 April 1912](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic). This notebook is intended to start learning ML so it will be based on some notebooks we have deemed interesting on the internet. References will be provided when needed."},{"metadata":{},"cell_type":"markdown","source":"![RMS Titanic][RMS]\n\n[RMS]: https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/RMS_Titanic_sea_trials_April_2%2C_1912.jpg/637px-RMS_Titanic_sea_trials_April_2%2C_1912.jpg \"RMS Titanic\"\n\nTo collect insights from data we will be applying the [CRISP-DM](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining \"Wikipedia CRISP-DM page\") (Cross Industry Standard Process for Data Mining) methodology.\n\n![CRISP-DM][MET]\n\n[MET]: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/599px-CRISP-DM_Process_Diagram.png \"CRISP-DM Methodology\"\n\nCRISP-DM methodology breaks the process of data mining into six phases:\n\n1. Business understanding: Understand what we need from the data.\n2. Data understanding: See how is the data formatted, what tools we need to operate with it and extract some basic insights (Exploratory Data Analysis, or EDA).\n3. Data preparation: Data wrangling and feature engineering.\n4. Modeling: Build a model to extract the answers needed from our data.\n5. Evaluation: Use a metric to compare our results and gain some feedback from our model to further improve it.\n6. Deployment: Submit our result to Kaggle."},{"metadata":{},"cell_type":"markdown","source":"<a id='Business_understanding'></a>\n## 3. Business understanding\n\n<a id='Background'></a>\n### 3.1. Background\n\nThe sinking of RMS Titanic is one of the most infamous sinkings in contemporary history. The RMS Titanic sank in the early hours of 15 April 1912, four days into her maiden's voyage after she struck an iceberg. Of the 2224 estimated people on board approximately 1500 people died in the accident, making it one of the most lethal peacetime maritimal disasters in history.\n\nOne of the reasons for such lethality was that there was not enough lifeboats for everyone. There was some element of luck involved in the survival probability, but there is evidence that some groups of people were more fortunate than others.\n\n<a id='Problem_description'></a>\n### 3.2. Problem description\n\nUsing some data from the RMS Titanic passenger list we have to predict whether or not a given passenger had survived the sinking or not. Kaggle is asking us to build a predictive model answering the following question: “what sorts of people were more likely to survive? using passenger data (ie name, age, gender, socio-economic class, etc)\". \n\n<a id='Target_audience'></a>\n### 3.3. Target audience\n\nThis notebook will be used to learn some machine learning techniques and how to manage a successful team. The primary audience will be ourselves, but when it is finished it will be published for anyone to read. Any constructive criticism will be welcome.\n\n<a id='Success_criteria'></a>\n### 3.4. Success criteria\n\nThe main metric employed to decide our model is successful or not is the accuracy of the model (i.e. in how many passengers the outcome was correctly predicted)."},{"metadata":{},"cell_type":"markdown","source":"<a id='Data_understanding'></a>\n## 4. Data understanding\n\nKaggle gives us two different csv files. One will be used to train our model (called train.csv) whereas the other will be used to test the model (unsurprisingly called test.csv). Test.csv does not contain information about the survival outcome of the passenger and we need to infer it from our model.\n\n<a id='Data_structure'></a>\n### 4.1. Structure of training and test sets\n\nTrain.csv contains data from 891 different passengers of the titanic with unique ID and full name, stating if they survived or not (column 'survived'), the ticket class ('Pclass'), sex ('Sex'), number of siblings/spouses aboard ('Sibsp'), number of parents/children aboard ('parch'), ticket number ('ticket'), fare ('fare'), cabin number aboard ('cabin') and port of embarcation ('embarked').\n\nSome notes from the documentation:\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\nPort of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n<a id='Submission_set'></a>\n### 4.2. Submission set\n\nThe submission set shall be composed of only two columns: The passenger id from the test set and a column indicating if that passenger survived or not.\n\n<a id='EDA'></a>\n### 4.3. Exploratory Data Analysis (EDA)\n\nFirst, some libraries will be imported:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd #pandas is a data analysis and manipulation tool.\nimport numpy as np #Package for scientific computing with Python.\nimport matplotlib.pyplot as plt #Python data visualization library.\nimport seaborn as sns #Python data visualization library based on matplotlib.\nimport re #regular expressions\n\n#Matlplotlib and seaborn options:\n%matplotlib inline\nsns.set(style=\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To study the full set, let's merge both datasets. In the case of the test data, the survival value will be set to NaN. The passenger ID is a unique number, different for every passenger on board. This ID will be set as the index of the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('../input/titanic/train.csv')\ndf_test=pd.read_csv('../input/titanic/test.csv')\n\nsurvive=np.empty((418,1))\nsurvive[:]=np.nan\ndf_test.insert(1,'Survived',survive)\n\ndf_full=df_train.append(df_test,ignore_index=True)\nprint('Dataset sizes:','\\n df_train: ',df_train.shape,'\\n df_test:  ',df_test.shape,'\\n df_full:  ',df_full.shape)\n\ndf_full.reset_index()\ndf_full.set_index('PassengerId', inplace=True) #Set the passenger id as index in both the test and train sets\n\ndf_full.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data type for each column is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the dataset there are both numerical variables (survived, pclass...) and categorical variables (sex, embarked). Categorical variables will be transformed to something the models can use using dummy variables. This means that the categorical variables will be transformed into dichotomic categorical variables in form of a number (1 for yes, 0 for no). This will be performed before the modeling section.\n\nSince survived is either 1 or 0, it will be transformed into an integer type (int64), as is Pclass. Name is comprised of different strings, as the Sex column, but that one will be transformed. Age is float, since there are some estimation in the ages and those are indicated with a \".5\". SibSp and Parch are people counts, so they are integers. Ticket is another string. Fare is a floating point number with sometimes a lot of decimals. This is due to the fact that the currency used in 1912 in the United Kingdom did not follow base-10 numbers. More information can be found in the [pre-decimal currency Wikipedia article](https://en.wikipedia.org/wiki/%C2%A3sd). Cabin is another string, and, finally, Embarked is a categorical variable with each letter indicating where the passenger boarded the ship.\n\nIn the next cell, a brief statistical description of both categorical and numerical variables can be seen"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3.1. Survival"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['Survived'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data for the survival outcome of the passengers is only known for the passengers in the training set (891 out of 1309 people). Only the 38% of passengers in the set have survived the sinking. In the following sections we will explore how survival correlates with other variables."},{"metadata":{},"cell_type":"markdown","source":"#### 4.3.2. Sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of females:\", df_full.loc[df_full.Sex=='female'].shape[0])\nprint(\"Number of males:\", df_full.loc[df_full.Sex=='male'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 466 females and 843 males in our dataset."},{"metadata":{},"cell_type":"markdown","source":"In the kaggle tutorial it is explained that gender played a key role in regards of survival probability, even creating a model that only looked at the sex of the passenger and then assigning the label survived=1 to females only, obtaining an impressive punctuation in the leaderboard. Let us explore how sex correlates with survival probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"rate_women=100 * df_train.loc[df_train.Sex == 'female'][\"Survived\"].sum()/df_train.loc[df_train.Sex=='female'].shape[0]\nrate_men=100 * df_train.loc[df_train.Sex == 'male'][\"Survived\"].sum()/df_train.loc[df_train.Sex=='male'].shape[0]\n\nprint(\"Percentage of women who survived:\", round(rate_women,2),'%')\nprint(\"Percentage of men who survived:\", round(rate_men,2),'%')\n\nfig1=sns.barplot(x=\"Sex\", y=\"Survived\", data=df_full)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 18.89 % of men survived, compared with the 74.2 % of women. Is it true the \"Women and children first\" saying? Apparently, in 1912 it was _en vogue_."},{"metadata":{},"cell_type":"markdown","source":"#### 4.3.3 Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['Age'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ther are 1046 values of 1309 possible values. This indicates that some age values are missing. The average age is less than 30 years old (29.9 years), with a standard deviation of 14.4 years. The youngest recorded passenger was a little older than 2 months (0.17 years) old, whereas the oldest passenger was 80 years old. More than 75% of the passengers were less than 40 years old. How are the age and the survival outcome correlated?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_ages=pd.DataFrame()\n#df_ages['Age']=pd.cut(df_train['Age'], 8)\n#df_ages['Survived']=df_train['Survived']\n#df_ages[['Age', 'Survived']].groupby(['Age'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2, ax2 = plt.subplots()\nlabels=['Survived','Not survived']\nfor a,lab in zip([df_full[df_full['Survived']==1]['Age'], df_full[df_full['Survived']==0]['Age']],labels):\n    sns.distplot(a, bins=range(0, 81, 10), ax=ax2, kde=False, label=lab)\nax.set_xlim([0, 80])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This histogram shows us how the survival probability correlates with age. We can see that is more probable to survive for people from 0 to 10 years old. In the other groups of age it is more probable to not survive than leaving the sinking boat alive. This is specially significant in the 20-40 year brackets."},{"metadata":{},"cell_type":"markdown","source":"How about we combine both sex and age and study the survival probabilities?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=sns.FacetGrid(df_full,hue='Survived',aspect=4,row='Sex')\nfig1.map(sns.kdeplot,'Age',shade=True)\nfig1.set(xlim=(0,df_full[\"Age\"].max()))\nfig1.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Survived',y='Age',hue='Sex',data=df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"For both sexes, the bulk of people surviving concentrates in the ages between 20 and 40 years old. There is a local maximum for young males. The survival distribution for females tends to be more uniform, with females surviving around all age brackets and showing a maximum at around 25 years of age. There is also a little peak at around 50 years old that is not shown for the male population."},{"metadata":{},"cell_type":"markdown","source":"#### Ticket Fare"},{"metadata":{},"cell_type":"markdown","source":"And in the following graph we can see the survival probability as a function of ticket fare, as a whole (first graph) and separated for male and female passengers."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig5=sns.FacetGrid(df_full,hue='Survived',aspect=4)\nfig5.map(sns.kdeplot,'Pclass',shade=True)#.set(yscale = 'log')\nfig5.set(xlim=(0,df_full[\"Pclass\"].max()))\nfig5.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3=sns.FacetGrid(df_full,hue='Survived',aspect=4,row='Sex')\nfig3.map(sns.kdeplot,'Fare',shade=True)#.set(yscale = 'log')\nfig3.set(xlim=(0,df_full[\"Fare\"].max()))\nfig3.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_full[df_full.Pclass==1]\nsns.distplot(df['Fare'],  kde=False, label='First class')\ndf=df_full[df_full.Pclass==2]\nsns.distplot(df['Fare'],  kde=False, label='Second class')\ndf=df_full[df_full.Pclass==3]\nsns.distplot(df['Fare'],  kde=False, label='Third class')\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Fare distribution per class')\nplt.xlabel('Fare (£)')\nplt.ylabel('Frecuency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Family Size"},{"metadata":{},"cell_type":"markdown","source":"We can also study how the survival probability correlates with having family members aboard the titanic. The survival distribution as a function of the variable Sibsp is as follows"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig5=sns.FacetGrid(df_full,hue='Survived',aspect=4)\nfig5.map(sns.kdeplot,'SibSp',shade=True)#.set(yscale = 'log')\nfig5.set(xlim=(0,df_full[\"SibSp\"].max()))\nfig5.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These plots show that the cost of the fare may be correlated with survival, as the non-survival probability is much higher in the low-paying fares than in the more expensive ones, for both male and female passengers."},{"metadata":{},"cell_type":"markdown","source":"#### Sex"},{"metadata":{},"cell_type":"markdown","source":"Here we can see, as stated in the tutorial, that the gender can be used as an indicator of survival probability, since more than 70% of the females on board survived, as opposed to a little less than 30% of the males. Next, we are going to check whether or not the social class (by means of classifying in terms of passenger class) plays an important role in the survival probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig8=sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=df_full)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People in first class have more survival rate, for both males and females, than the other two ticket classes. It is possible that the social class can be used as another feature of our survival prediction model. What about the port of embark?"},{"metadata":{},"cell_type":"markdown","source":"### Port of Embark"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig9=sns.barplot(x=\"Embarked\",y=\"Survived\",data=df_full)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have now a lot of information to start working on the model. "},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"We need to convert some of the categorical variables in order to feed them to our model. First, we will convert the variable 'Sex'. Since in the records from that era the sex was treated as a binary dichotomic value (you were either male or female) we will use that in our model as well, with 1 being male and 0 female."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['Sex'] = df_full['Sex'].map({'male':1,'female':0})\ndf_full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Title information"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_title(name):\n\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n\t# If the title exists, extract and return it.\n\tif title_search:\n\t\treturn title_search.group(1)\n\treturn \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['Title'] = df_full['Name'].apply(get_title)\n\ndf_full['Title'] = df_full['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                            'Major', 'Rev', 'Sir', 'Jonkheer'], 'Noble')   \ndf_full['Title'] = df_full['Title'].replace('Don', 'Mr')\ndf_full['Title'] = df_full['Title'].replace(['Mlle','Ms'], 'Miss')\ndf_full['Title'] = df_full['Title'].replace(['Mme','Dona'], 'Mrs')\n\nmale_dr_filter = (df_full.Title == 'Dr') & (df_full.Sex == 1)\nfemale_dr_filter = (df_full.Title == 'Dr') & (df_full.Sex == 0)\n\ndf_full.loc[male_dr_filter, ['Title']] = 'Mr'\ndf_full.loc[female_dr_filter, ['Title']] = 'Mrs'\n\nprint(pd.crosstab(df_full['Title'], df_full['Sex']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Segmentation of age groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_age = pd.DataFrame({'Age': df_full.Age, 'Survived': df_full.Survived}).dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_age = preprocessing.StandardScaler().fit_transform(X_age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_age=pd.DataFrame()\n#X_age=df_full.loc[:891,['Age','Survived']].copy()\n#X_age['Survived']=df_full[:891].Survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans #K Means clustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sum_squared_distances = []\nK = range(1,16)\nfor k in K:\n    km = KMeans(n_clusters=k,init='k-means++')\n    km = km.fit(X_age)\n    Sum_squared_distances.append(km.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(K, Sum_squared_distances, 'bx-')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow Method for optimal number of clusters')\nplt.savefig('elbow.png',dpi=300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_cl = 4\nkmeans = KMeans(n_clusters=n_cl, init='k-means++')\nkmeans.fit(X_age)\n\nX_age['Label']=kmeans.labels_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the decision boundary\n# See http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\nplt.figure(figsize=(10,5))\nh = 0.01\nx_min, x_max = X_age['Age'].min() - h, X_age['Age'].max() + h\ny_min, y_max = X_age['Survived'].min() - h, X_age['Survived'].max() + h\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Predict the age cluster for each point in a mesh\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\ncmap = sns.cubehelix_palette(start=2.8, rot=.1, as_cmap=True)\nZ = Z.reshape(xx.shape)\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=cmap, aspect='auto')\n\n# Plot the ages\nsns.scatterplot(x='Age', y='Survived', hue='Label', data=X_age, palette=cmap)\n\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w')\nplt.yticks([0, 1])\nplt.title(\"Age clusters and decision boundaries\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert K-means clusters to age bands\nage_bands = []\nfor k in range(n_cl):\n    age_bands.append(xx[Z==k].min())\n\n# Since the clusters are not sorted we sort the intervals\nage_bands.sort()\n\n# Set the lower bound of the first interval to 0\nage_bands[0] = 0\n\n# Set the higher bound of the last interval to infinite just in case there are older older passengers in the test set\nage_bands.append(np.inf)\n\n# Convert list to numpy array\nprint(\"Age bands: {}\".format(np.array(age_bands)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Age"},{"metadata":{},"cell_type":"markdown","source":"Now we can fill the missing values in some of the variables. The age seems to be missing in some of the entries. To fill the data, we will calculate the median as a function of sex and passenger class."},{"metadata":{"trusted":true},"cell_type":"code","source":"ages_table=df_full[:891].groupby(['Title','Pclass'])['Age'].median()\nages_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['Age']=df_full['Age'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.loc[(df_full.Age == 0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ind,row in df_full[df_full['Age']==0].iterrows():\n    df_full.loc[ind,'Age']=ages_table[row.Title][row.Pclass]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.loc[(df_full.Age == 0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['AgeBand'] = pd.cut(df_full['Age'], age_bands)\n\ndf_full.groupby('AgeBand')['Survived'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.AgeBand.dtype","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Family Size"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['FamilySize'] = df_full['SibSp'] + df_full['Parch'] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_full['IsAlone'] = df_full.FamilySize.apply(lambda x: 1 if x == 1 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fare"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['TicketOcurr'] = df_full.groupby('Ticket')['Ticket'].transform('size')\ndf_full['FarePerPerson'] = df_full['Fare']/df_full['TicketOcurr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full[df_full[['FarePerPerson']].isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fares_table=df_full[:891].groupby('Pclass')['FarePerPerson'].median()\nprint(fares_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['IsAlone'] = df_full.TicketOcurr.apply(lambda x: 1 if x == 1 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full[df_full[['Fare']].isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df, fare in zip([df_full],[fares_table]):\n    for cls in np.unique(df.Pclass):\n        df.loc[(df['FarePerPerson'].isnull()) & (df['Pclass']==cls),'FarePerPerson']=fare.loc[cls]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full[df_full.FarePerPerson.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_full[df_full.Pclass==1]\nsns.distplot(df['FarePerPerson'],  kde=False, label='First class')\ndf=df_full[df_full.Pclass==2]\nsns.distplot(df['FarePerPerson'],  kde=False, label='Second class')\ndf=df_full[df_full.Pclass==3]\nsns.distplot(df['FarePerPerson'],  kde=False, label='Third class')\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Fare distribution per class')\nplt.xlabel('Fare per person (£)')\nplt.ylabel('Frecuency')\nplt.savefig('Fares.png',dpi=300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full[df_full['Fare']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Port of Embarkment"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full[df_full[['Embarked']].isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_nan=df_full[df_full[['Embarked']].isnull().any(axis=1)].index\nport_samp=list(df_train['Embarked'].sample(len(num_nan),replace=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(num_nan)):\n    df_full.loc[num_nan[i],'Embarked']=port_samp[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full[df_full[['Embarked']].isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full=pd.get_dummies(df_full, columns=[\"Embarked\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making dummies of categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full=pd.get_dummies(df_full, columns=[\"Pclass\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full=pd.get_dummies(df_full, columns=[\"Title\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full=pd.get_dummies(df_full, columns=[\"AgeBand\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate the correlation matrix for the train set\ncorr_full=df_full.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_10=plt.figure(figsize=(10,10))\nsns.heatmap(corr_full, annot=True)\nplt.title(\"Titanic survivor correlation matrix heatmap\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features = Sex\tIsAlone\tFarePerPersonEmbarked_CEmbarked_QEmbarked_SPclass_1Pclass_2Pclass_3Title_MasterTitle_MissTitle_MrTitle_MrsTitle_NobleAgeBand_(0.0, 13.16]AgeBand_(13.16, 27.67]AgeBand_(27.67, 43.52]AgeBand_(43.52, inf]\ndata = df_full.drop(['Survived','Name','Age','SibSp','Parch','Ticket','Fare','Cabin','FamilySize','TicketOcurr','FarePerPerson'],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df_train.Survived # being df_train the original train data we imported in the beginning \ntrain = data[0:891]        # \"original\" train set containing transformed/selected features\ntest = data[891:]    # \"original\" test set containing transformed/selected features ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = preprocessing.StandardScaler().fit_transform(train)\ny = target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rbf = svm.SVC(kernel='rbf')\nclf_rbf.fit(X_train, y_train) \nyhat = clf_rbf.predict(X_test)\nmat=confusion_matrix(y_test, yhat)\nsns.heatmap(mat.T, cmap=(\"Blues\"), square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=['Not survived','Survived'],\n            yticklabels=['Not survived','Survived'])\nplt.xlabel('true label')\nplt.ylabel('predicted label');\nprint(classification_report(y_test, yhat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"True or false indicated if the classifier predicted the class correctly. Positive or negative indicates if the classifier predicted the desired class."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve , ShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ntitle = r\"Learning Curves (SVM, RBF kernel)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(clf_rbf, title, X, y, ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=-1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Tuned SCV Accuracy: {round(accuracy_score(y_test, yhat), 2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val=validation\nX_val=preprocessing.StandardScaler().fit(train).transform(X_val)\ny_val=clf_rbf.predict(X_val)\n\n#print(f'Tuned SCV Accuracy: {round(accuracy_score(y_test, y_val), 2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n       \"PassengerId\": df_test[\"PassengerId\"],\n       \"Survived\": y_val\n   })\n\nprint(submission.shape)\nsubmission.to_csv('titanic_svc.csv', index=False)\n\n#kaggle competitions submit -c titanic -f titanic_svc_test.csv -m \"Test of automatic submission\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimized SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_val=[0.01, 0.05, 0.1, 0.5, 1, 10]\n\nparam_grid = [\n    {'kernel': ['linear'], 'C': C_val},\n    {'kernel': ['poly'], 'C': C_val, 'degree': [2, 3, 4, 5], 'gamma': ['scale', 'auto']},\n    {'kernel': ['rbf'], 'C': C_val, 'gamma': ['scale', 'auto']},\n    {'kernel': ['sigmoid'], 'C': C_val, 'gamma': ['scale', 'auto']}\n    ]\n\n#param_grid = [\n#    {'kernel': ['poly'], 'C': [0.05, 0.1, 0.5, 1, 5], 'degree': [2, 3, 4]},\n#    {'kernel': ['rbf'], 'C': [0.05, 0.1, 0.5, 1, 5]},\n#    {'kernel': ['sigmoid'], 'C': [0.05, 0.1, 0.5, 1, 5]}\n#    ]\n\n#param_grid = [{'kernel': ['rbf'], 'C': [0.1, 0.5]}\n#             ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = svm.SVC()\nclf_svc_grid = GridSearchCV(svc, param_grid, cv=5)\nbest_model = clf_svc_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Print the value of best Hyperparameters\nprint('Best kernel:', best_model.best_estimator_.get_params()['kernel'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\nprint('Best degree:', best_model.best_estimator_.get_params()['degree'])\nprint('Best gamma:', best_model.best_estimator_.get_params()['gamma'])\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = r\"Learning Curves (SVM, RBF kernel)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\nplot_learning_curve(best_model, title, X, y, ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=-1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model.predict(X_test)\n## df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n## print(df)\n## print('\\n')\nprint(f'Tuned SCV Accuracy: {round(accuracy_score(y_test, y_pred), 2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {'criterion': ['gini'], 'n_estimators': [100], 'max_depth': list(np.arange(5,11)) + [None], \n     'min_samples_split': np.arange(2, 6), 'min_samples_leaf': np.arange(1, 6),\n     'max_features': ['sqrt', 'log2', None], 'random_state': [4]},\n    {'criterion': ['entropy'], 'n_estimators': [100], 'max_depth': list(np.arange(5,11)) + [None], \n     'min_samples_split': np.arange(2, 6), 'min_samples_leaf': np.arange(1, 6),\n     'max_features': ['sqrt', 'log2', None], 'random_state': [4]}\n    ] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nclf_rf_grid = GridSearchCV(rf, param_grid, cv=5)\nbest_model = clf_rf_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Criterion:', best_model.best_estimator_.get_params()['criterion'])\nprint('Number of estimators:', best_model.best_estimator_.get_params()['n_estimators'])\nprint('Max depth:', best_model.best_estimator_.get_params()['max_depth'])\nprint('Min samples split:', best_model.best_estimator_.get_params()['min_samples_split'])\nprint('Min samples leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\nprint('Max features:', best_model.best_estimator_.get_params()['max_features'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Random Forest Accuracy: {round(accuracy, 2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val = validation\nX_val=preprocessing.StandardScaler().fit(train).transform(X_val)\ny_val=best_model.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n       \"PassengerId\": df_test[\"PassengerId\"],\n       \"Survived\": y_val\n   })\nsubmission.to_csv('titanic_RF_best.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf_best_hand = RandomForestClassifier(criterion='entropy', n_estimators=100, max_depth=None, min_samples_split=4, \n                                 #min_samples_leaf=3, max_features=None, random_state=4)\n\ntitle = r\"Random Forest Classifier\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(best_model, title, X, y, ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=-1)\n#plot_learning_curve(rf_best_hand, title, X, y, ylim=(0.7, 1.01),\n#                    cv=cv, n_jobs=-1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To do:\n\n- Hacer seleccion de algoritmos (sin XGBoost)\n- Optimizar esos algoritmos\n- Usar esos algoritmos para dar predicciones\n- Feed predicciones a XGBoost (stacking)"},{"metadata":{},"cell_type":"markdown","source":"### Selecting the best ML Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 2 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits= NFOLDS, shuffle= True, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n# Class to extend XGboost classifer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,)) #array vacio de longitud igual al numero de el de train en dataset\n    oof_test = np.zeros((ntest,)) #lo mismo con el test\n    oof_test_skf = np.empty((NFOLDS, ntest)) #tantas filas como KFolds y columnas igual a elemetos de test en dataset\n\n    for i, (train_index, test_index) in enumerate(kf): #i es el iterable de kfolds, train index y test index los valores que kfolds asigna a los elementos del train set\n        x_tr = x_train[train_index] #train del train set (crossvalidation)\n        y_tr = y_train[train_index] #etiquetas del train cross validation\n        x_te = x_train[test_index] # test del train set (crossvalidation)\n\n        clf.train(x_tr, y_tr) #entrena modelo con train cv\n\n        oof_train[test_index] = clf.predict(x_te) #predice valores de test crossvalidation\n        oof_test_skf[i, :] = clf.predict(x_test) #valores predichos del testset de verdad\n\n    oof_test[:] = oof_test_skf.mean(axis=0) #media en columnas de la prediccion de todos los entrenamientos del modelo con kfolds\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) #devuelve los valores de test crossvalidation y del test de verdad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing necessary modules for training and evaluation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Data preprocessing and splitting\nscaler = StandardScaler()\nX = scaler.fit_transform(train)\ny = target\n\n#Number of K folds\nn_kfolds=10\n\n# Number of estimators for tree-based ensembles\nnum_estimators = 100\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n#print ('Train set:', X_train.shape,  y_train.shape)\n#print ('Test set:', X_test.shape,  y_test.shape)\n\n# Calculating score for a bunch of algortihms\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\n\nrand_state=SEED\n\nmodels = [LogisticRegression(\n             random_state=rand_state),\n          Perceptron(\n              random_state=rand_state), \n          SGDClassifier(\n              random_state=rand_state), \n          SVC(\n              random_state=rand_state), \n          KNeighborsClassifier(\n              ), \n          GaussianNB(\n              ),\n          DecisionTreeClassifier(\n              random_state=rand_state), \n          RandomForestClassifier(\n              random_state=rand_state,\n              n_estimators=num_estimators),\n          ExtraTreesClassifier(\n              random_state=rand_state,\n              n_estimators=num_estimators),\n          AdaBoostClassifier(\n              random_state=rand_state,\n              n_estimators=num_estimators),\n          GradientBoostingClassifier(\n              random_state=rand_state, \n              n_estimators=num_estimators)\n         ]\n\nmodel_name = []\nacc_test = []\nacc_train = []\ncv_scores = []\n\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    model_name.append(model.__class__.__name__)\n    acc_test.append(model.score(X_test, y_test))\n    acc_train.append(model.score(X_train, y_train))\n    cv_scores.append(cross_val_score(model, X, y, cv=5))\n\nresults = pd.DataFrame({\n    'Model': model_name,\n    'CvScore' : cv_scores,\n    'TestScore': acc_test,\n    'TrainScore': acc_train\n    })\n\n\nresults.insert(1, 'CVMeanScore', np.mean(results['CvScore'].tolist(), axis=1))\nresults.drop(['CvScore'],axis=1,inplace=True)\n\n#by='TestScore'\nresults.sort_values(by='CVMeanScore', ascending=False, ignore_index=True, inplace=True)\n#print(results)\n\nprint('The five best ML models for this problem are:\\n') \nprint(results.iloc[:5,0:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}